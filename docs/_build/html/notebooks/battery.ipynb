{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnCortex - Battery Arbitrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set() \n",
    "import typing as t \n",
    "import gym\n",
    "from gym import spaces\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "#import the encortex library and all the required dependencies\n",
    "from encortex.backend import DFBackend\n",
    "from encortex.env import EnCortexEnv\n",
    "from encortex.logger import get_experiment_logger\n",
    "from encortex.utils.data_loaders import load_data\n",
    "from encortex.data import MarketData\n",
    "from encortex.contract import Contract\n",
    "from encortex.decision_unit import DecisionUnit\n",
    "from encortex.grid import Grid\n",
    "from encortex.sources import Battery, BatteryAction\n",
    "\n",
    "from dfdb import create_in_memory_db\n",
    "\n",
    "from encortex.environments.battery_arbitrage_env import BatteryArbitrageScenarioEnv\n",
    "from encortex.optimizers.battery_arbitrage_optimizer import DRLBattOpt, MILPBattOpt, Optimizer\n",
    "from encortex.datasets.grid import CaliforniaPricesEmissionsData, UKPricesEmissionsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs from User: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the type of optimization to be used:\n",
    "milp_flag = True #if False use RL\n",
    "solver = [\"ort\"] #the algorithm to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide optimization weights for the objectives\n",
    "weight_emission = 1.0\n",
    "weight_price = 1.0\n",
    "weight_degradation = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "run experiments for the following battery configurations\n",
    "elements in the list denote different batteries/battery configurations to be used in the scenario together (here just 1 element indicating 1 battery being used)\n",
    "'''\n",
    "#the battery capacity (in kWh)\n",
    "storage_capacity = [10.]\n",
    "\n",
    "#here charging and discharging efficiency (in %) taken the same/ if different take it differently \n",
    "efficiency=[1.]\n",
    "\n",
    "#the maximum discharge (in %) percentage that can happen at a time, here 90% \n",
    "depth_of_discharge = [90.] \n",
    "\n",
    "#the minimum state of charge of the battery, below which the battery should not be explored\n",
    "soc_minimum = [0.1]\n",
    "\n",
    "#battery decision time steps  \n",
    "timestep = [np.timedelta64(\"1\",\"h\")]\n",
    "\n",
    "#whether to have degradation model in place or not for the batteries \n",
    "degradation_flag = weight_degradation > 0 \n",
    "\n",
    "#the battery capacity reduction percentage due to degradation, below which if capacity reduces due to overuse, battery does not stay at good optimal health\n",
    "min_battery_capacity_factor = [0.8] \n",
    "\n",
    "#the battery replacement cost (in $/kWh)\n",
    "battery_cost_per_kWh = [200.] \n",
    "\n",
    "#after every charge-discharge cycles over a certain period, the battery capacity reduced by the reduction coefficienct \n",
    "reduction_coefficient = [0.99998] \n",
    "\n",
    "# the period after which battery degrades\n",
    "degradation_period_in_days = [7.] \n",
    "\n",
    "#battery actions\n",
    "action = [BatteryAction(\"CHARGE_IDLE_DISCHARGE\",\"actions of the battery\",spaces.Discrete(3),True,)] \n",
    "\n",
    "# initial state of charge of the battery to run the test experiments\n",
    "# the flag initiates random initial state of charge of the battery during training runs/experiments to avoid overfitting\n",
    "soc_initial = [0.5] \n",
    "test_flag = [False] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating components from the framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/06 13:19:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , forecast_carbon_emissions, forecast_prices, timestamps\n",
      " Schema: _c0, forecast_carbon_emissions, forecast_prices, timestamps\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/rluser/vb_cortex/data/forecast_grid_data.csv\n",
      "22/07/06 13:19:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , actual_carbon_emissions, actual_prices, timestamps\n",
      " Schema: _c0, actual_carbon_emissions, actual_prices, timestamps\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/rluser/vb_cortex/data/actual_grid_data.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "instantiate battery objects into a list based on the no. of batteries/elements provided in the list of configuration parameters \n",
    "'''\n",
    "batteries = []\n",
    "for ele in range(len(storage_capacity)):\n",
    "    battery = Battery(\n",
    "        timestep=timestep,\n",
    "        name=\"Li-Ion Battery\",\n",
    "        id=ele,\n",
    "        description=\"Li-Ion Battery\",\n",
    "        storage_capacity=storage_capacity[ele],\n",
    "        charging_efficiency=efficiency[ele],\n",
    "        discharging_efficiency=efficiency[ele],\n",
    "        soc_initial=soc_initial[ele],\n",
    "        depth_of_discharge=depth_of_discharge[ele],\n",
    "        soc_minimum=soc_minimum[ele],\n",
    "        degradation_flag=degradation_flag,\n",
    "        min_battery_capacity_factor=min_battery_capacity_factor[ele],\n",
    "        battery_cost_per_kWh=battery_cost_per_kWh[ele],\n",
    "        reduction_coefficient=reduction_coefficient[ele],\n",
    "        degradation_period=degradation_period_in_days[ele],\n",
    "        test_flag=test_flag[ele],\n",
    "        action=action[ele],\n",
    "    )\n",
    "    batteries.append(battery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rluser/anaconda3/envs/encortex/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "training data is not required for MILP, if working on RL training data is required, read the data from the dataloader, \n",
    "parse it to the backend of MArketData and feed it to the Grid class to instantiate a Grid object for training\n",
    "'''\n",
    "\n",
    "'''\n",
    "load forecast and actual data for training - \n",
    "The CaliforniaPricesEmissionsData takes in 3 user-specific arguments:\n",
    "    1. train: A flag saying whether training/test data to load\n",
    "    2. forecasts: A flag saying whether experiments are to be run on forecasts/actuals\n",
    "    3. forecast_type: A string specifyng the type of forecast:\n",
    "        a) noise : Adding noise to the actual values and treating that as forecasts\n",
    "        b) smoothing : Smoothing the actual values and using that as forecasts\n",
    "        c) yesterdays : assuming yesterday's actual data as forecasts for today's data\n",
    "'''\n",
    "forecast_df = UKPricesEmissionsData(train=True, forecasts=True, )\n",
    "actual_df = UKPricesEmissionsData(train=True, forecasts=False)\n",
    "\n",
    "print(forecast_df.data.head(),'\\n', actual_df.data.head())\n",
    "\n",
    "forecast_df.data[['emissions', 'prices']]  = forecast_df.data[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "actual_df.data[['emissions', 'prices']]  = actual_df.data[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "\n",
    "#parse the training data to the MarketData backend\n",
    "grid_data = MarketData.parse_backend(\n",
    "    len(storage_capacity) + 2,\n",
    "    True,\n",
    "    len(storage_capacity) + 2,\n",
    "    len(storage_capacity) + 2,\n",
    "    np.timedelta64(\"5\", \"m\"),\n",
    "    price_forecast=DFBackend(forecast_df.data['prices'], forecast_df.data['timestamps']),\n",
    "    \n",
    "    price_actual=DFBackend(actual_df.data['prices'], actual_df.data['timestamps']),\n",
    "    carbon_emissions_forecast=DFBackend(forecast_df.data['emissions'], forecast_df.data['timestamps']),\n",
    "    carbon_emissions_actual=DFBackend(actual_df.data['emissions'], actual_df.data['timestamps']),\n",
    "    carbon_prices_forecast=DFBackend(forecast_df.data['prices'], forecast_df.data['timestamps']),\n",
    "    carbon_prices_actual=DFBackend(actual_df.data['prices'], actual_df.data['timestamps']),\n",
    "    volume_forecast=DFBackend(None, None),\n",
    "    volume_actual=DFBackend(None, None),\n",
    ")\n",
    "\n",
    "#instantiate a training grid Object by feeding in the parse training data as an argument to the Grid class\n",
    "grid = Grid(\n",
    "    timestep,\n",
    "    \"Grid\",\n",
    "    len(storage_capacity) + 2,\n",
    "    \"Simple Market as a Grid\",\n",
    "    \"*/5 * * * *\",\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    grid_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Decision units for the problem statement:\n",
    "\n",
    "A [decision unit](../encortex/encortex.decision_unit.rst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate the decision unit (both for training and testing) by creating contracts between the grid and the battery\n",
    "def creating_decision_units(batteries, grid, forecast_df):\n",
    "    contracts = []\n",
    "    for battery in batteries:\n",
    "        contracts.append(Contract(grid,battery))\n",
    "    decision_unit = DecisionUnit(contracts)\n",
    "    decision_unit.generate_schedule(\n",
    "        current_reference_time=np.datetime64(pd.Timestamp(forecast_df.data['timestamps'][0]))\n",
    "    )\n",
    "    return decision_unit\n",
    "\n",
    "decision_unit = creating_decision_units(batteries, grid, forecast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Store Results in a dataframe and then later to csv format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dump results into dataframes for later visualizations - \n",
    "    1. battery_soc_list: stores the current list of state of charge values for MILP\n",
    "    2. action_list : list of actions: charging/discharging/idle taken by the optimizer for a step\n",
    "    3. power_list: power associated with the action taken by the optimization algorithm\n",
    "    4. carbon_intensity_list: Actual carbon emission intensity values in mTCO2eq/kWh\n",
    "    5. price_intensity_list: Actual price values in $\n",
    "    6. reward_list: List of rewards received for taking actions in particular states\n",
    "    7. carbon_savings_forecast_list : Carbon savings done due to the action taken for a particular state at a certain timestamp\n",
    "    8. price_savings_forecast_list : Price savings due to the action taken for a particular state at a certain timestamp \n",
    "'''\n",
    "def create_dataframe(\n",
    "    battery_soc_list, \n",
    "    action_list, \n",
    "    power_list, \n",
    "    carbon_intensity_list,\n",
    "    price_intensity_list,\n",
    "    carbon_savings_list,\n",
    "    price_savings_list,\n",
    "    reward_list, \n",
    "    carbon_savings_forecast_list, \n",
    "    price_savings_forecast_list\n",
    "):\n",
    "    df=pd.DataFrame()\n",
    "    df.insert(loc=0, column='Current_SOC', value=battery_soc_list)\n",
    "    df.insert(loc=1, column='Predicted_Action', value=action_list)\n",
    "    df.insert(loc=2, column='Predicted_Power_Action', value=power_list)\n",
    "    df.insert(loc=3, column='Carbon_emissions', value=carbon_intensity_list)\n",
    "    df.insert(loc=4, column='Carbon_savings', value=carbon_savings_list)\n",
    "    df.insert(loc=5, column='Forecast_Carbon_savings', value=carbon_savings_forecast_list)\n",
    "    df.insert(loc=6, column='Price_emissions', value=price_intensity_list)\n",
    "    df.insert(loc=7, column='Price_savings', value=price_savings_list)\n",
    "    df.insert(loc=8, column='Forecast_Price_savings', value=price_savings_forecast_list)\n",
    "    df.insert(loc=9, column='Reward', value=reward_list)  \n",
    "    return df\n",
    "\n",
    "#store the results into results folder\n",
    "dir = os.getcwd()\n",
    "dir_path = os.path.join(dir, 'results_UK_forecasts/')\n",
    "\n",
    "if not os.path.isdir(dir_path):\n",
    "    os.mkdir(dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instantiate an environment object from the scenario specific environment class \n",
    "'''\n",
    "if milp_flag:\n",
    "    step_time_diff = np.timedelta64(\"1\", \"D\")\n",
    "else:\n",
    "    step_time_diff = np.timedelta64(\"1\", \"h\")\n",
    "    \n",
    "env = BatteryArbitrageScenarioEnv(\n",
    "    decision_unit,\n",
    "    start_time=forecast_df.data['timestamps'][0],\n",
    "    timestep=np.timedelta64(\"1\", \"h\"),\n",
    "    step_time_difference=step_time_diff,\n",
    "    horizon=np.timedelta64(\"1\", \"D\"),\n",
    "    seed=40,\n",
    "    weight_emission=weight_emission,\n",
    "    weight_degradation=weight_degradation,\n",
    "    weight_price=weight_price,\n",
    "    milp_flag=milp_flag,\n",
    "    train_flag=True,\n",
    "    exp_logger=get_experiment_logger('wandb'),\n",
    "    logging_interval = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline for the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting a seed for reproducibility of experiment results:\n",
    "pl.seed_everything(40)\n",
    "seed = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Pipeline for RL:\n",
    "The code in this cell helps to train a RL model, but there could be issues in trying it out in the jupyter cell. \n",
    "Hence we also provide a separate training script namely training_RL.py, try that out if the jupyter cell does not work.\n",
    "\n",
    "During testing just the load the model saved from the training script\n",
    "'''\n",
    "if not milp_flag:\n",
    "\n",
    "    #instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = DRLBattOpt(\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    print(\"...... Starting Training .......\")\n",
    "    model = opt(\n",
    "        env,\n",
    "        train_flag=True,\n",
    "    )\n",
    "    print(\"------Training Completed--------\")\n",
    "    \n",
    "    #save the model into a folder\n",
    "    dir = os.getcwd()\n",
    "    dir_path = os.path.join(dir, 'model_checkpoints/')\n",
    "\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "\n",
    "    opt.save(dir_path, \"Trained_RL_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mixed integer linear programming (MILP) does not require training, hence the code for the MILP section is shown directly while testing\n",
    "\n",
    "### Testing Pipeline for the algorithms : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The test pipeline:\n",
    "def testing(model, env: BatteryArbitrageScenarioEnv, opt: Optimizer, milp_flag: bool):\n",
    "    \n",
    "    #for logging results into csv files\n",
    "    carbon_intensity_list=[]\n",
    "    price_intensity_list=[]\n",
    "    battery_soc_list=[]\n",
    "    action_list=[]\n",
    "    power_list=[]\n",
    "    reward_list=[]\n",
    "    carbon_savings_list=[]\n",
    "    price_savings_list=[]\n",
    "    carbon_savings_forecast_list=[]\n",
    "    price_savings_forecast_list=[]\n",
    "    \n",
    "    #For testing initiate the battery with 50% charge always (initial state of charge of the battery during test experiments : 0.5)\n",
    "    for batt in env.decision_unit.storage_entities:\n",
    "        batt: Battery\n",
    "        batt.current_soc = 0.5\n",
    "        batt.test = True\n",
    "        \n",
    "    #Reset the environment in the beginning and get the state values\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    net_reward = 0\n",
    "    steps=0\n",
    "    \n",
    "    #run the episode unless done\n",
    "    while not done:\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"State: \", len(state))\n",
    "        print(\"Step no : \", steps)\n",
    "        print(\"Battery SOC : \", env.decision_unit.storage_entities[0].current_soc)\n",
    "        #storing the actual values of emissions & prices in the list for logging purpose\n",
    "        for grid in env.decision_unit.markets:\n",
    "            grid:Grid\n",
    "            carbon_intensity_list+=list(grid.data.carbon_emissions_actual[env.time, env.time+env.step_time_difference].reshape(-1))\n",
    "            price_intensity_list+=list(grid.data.carbon_prices_actual[env.time, env.time+env.step_time_difference].reshape(-1))\n",
    "         \n",
    "        #storing the state of charge of the batteries in a list for logging purpose:\n",
    "        for batt in env.decision_unit.storage_entities:\n",
    "            batt: Battery\n",
    "            battery_soc_list+=[batt.current_soc]\n",
    "             \n",
    "        if milp_flag:\n",
    "            #In MILP first the values are passed as a decision variable/ Affine Expression - the train flag signifies that\n",
    "            env.train_flag = True\n",
    "            \n",
    "            #model called to solve the objective defined in the environment based on the constraints from the framework abstractions\n",
    "            model = opt(train_flag=True)\n",
    "            battery_actions = opt.predict(env)\n",
    "            \n",
    "            #get the numeric action values as the predicted action results and hence switch off the train flag\n",
    "            env.train_flag = False\n",
    "            \n",
    "            for batt in env.decision_unit.storage_entities:\n",
    "                batt: Battery\n",
    "                action_list+=list(battery_actions[batt.id]['Dt']-battery_actions[batt.id]['Ct']+1)\n",
    "                power_list+=list((battery_actions[batt.id]['Dt']-battery_actions[batt.id]['Ct'])*batt.max_discharging_power)\n",
    "        else:\n",
    "            #In RL, since training is already done, just load the model to predict the actions based on the current state\n",
    "            action = model.predict(state)[0]\n",
    "\n",
    "            #transform the actions similar to the environment-specific action transformation for uniformity\n",
    "            battery_actions = {}\n",
    "            for batt in env.decision_unit.storage_entities:\n",
    "                \n",
    "                #storing the power values into a list for logging purpose\n",
    "                power_list.append((action-1)*batt.max_discharging_power)\n",
    "                \n",
    "                # battery_actions[batt.id] = {}\n",
    "                # battery_actions[batt.id][\"time\"] = env.time\n",
    "                # battery_actions[batt.id][\"action\"] = action\n",
    "\n",
    "            # storing actions taken into a list for logging purpose:\n",
    "            action_list+=[action]\n",
    "            battery_actions = action\n",
    "        \n",
    "        #Step to the next_state, based on the actions predicted by the optimizers, getting a reward and indicating whether the episode completed or not\n",
    "        # print(\"Battery Actions:\", battery_actions)\n",
    "        next_state, reward, done, info = env.step(battery_actions)\n",
    "        \n",
    "        #storing the rest of the state of charges in milp of the batteries in a list for logging purpose:\n",
    "        if milp_flag:\n",
    "            for batt in env.decision_unit.storage_entities:\n",
    "                batt: Battery\n",
    "                battery_soc_list+=info[batt.id]['soc_list'][:-1]\n",
    "        \n",
    "        #storing reward values in a list for logging purpose:\n",
    "        if milp_flag:\n",
    "            reward_list+=[0]*(int(env.step_time_difference / env.timestep)-1)\n",
    "        reward_list+=[reward]\n",
    "        \n",
    "        net_reward += reward\n",
    "        state = next_state\n",
    "        print(\"Total reward\", net_reward)\n",
    "        \n",
    "        #storing the savings values in the lists for logging purposes\n",
    "#         print(env.carbon_savings_list)\n",
    "        carbon_savings_list.append(env.carbon_savings_list)\n",
    "        price_savings_list.append(env.price_savings_list)\n",
    "        carbon_savings_forecast_list.append(env.carbon_savings_forecast_list)\n",
    "        price_savings_forecast_list.append(env.price_savings_forecast_list)\n",
    "        \n",
    "        steps+=1\n",
    "    return net_reward, reward_list, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list[0], price_savings_list[0], carbon_savings_forecast_list[0], price_savings_forecast_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for MILP to generate results on the training dataset: \n",
    "if milp_flag:\n",
    "\n",
    "    # instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = MILPBattOpt(\n",
    "        env=env, objective=weight_price, solver=solver[0], seed=seed\n",
    "    )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    # provide the model to the environment so as to prepare the constraints and objectives\n",
    "    env.set_model(model)\n",
    "\n",
    "    # test the MILP model\n",
    "    print(\"-------Producing results on training set--------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    traindf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    traindf.to_csv(dir_path+\"traindf_MILP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Trained RL model on the training data set first:\n",
    "if not milp_flag: \n",
    "\n",
    "    opt = DRLBattOpt(\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    #load the model first, if trained from the python script training_RL.py \n",
    "    opt.load('model_checkpoints_UK/', 'Trained_RL_model')\n",
    "    \n",
    "    \n",
    "    print(\"-------Producing results on training set--------\")\n",
    "    #test the RL model on the training set\n",
    "    net_rewardt,rewardlist,  battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(opt.model, env, opt, milp_flag)\n",
    "\n",
    "    # dump the training results into csv file\n",
    "    traindf= create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list,rewardlist,  carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    traindf.to_csv(dir_path+\"traindf_RL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read test data having emissions & prices values of California dataset\n",
    "#load forecast and actual data for testing/inference \n",
    "forecast_df = UKPricesEmissionsData(train=False, forecasts=True) \n",
    "actual_df = UKPricesEmissionsData(train=False, forecasts=False)\n",
    "\n",
    "forecast_df.data[['emissions', 'prices']]  = forecast_df.data[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "actual_df.data[['emissions', 'prices']]  = actual_df.data[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "\n",
    "#parse the test data to the MarketData backend\n",
    "grid_data = MarketData.parse_backend(\n",
    "    len(storage_capacity) + 2,\n",
    "    True,\n",
    "    len(storage_capacity) + 2,\n",
    "    len(storage_capacity) + 2,\n",
    "    np.timedelta64(\"5\", \"m\"),\n",
    "    price_forecast=DFBackend(forecast_df.data['prices'], forecast_df.data['timestamps']),\n",
    "    price_actual=DFBackend(actual_df.data['prices'], actual_df.data['timestamps']),\n",
    "    carbon_emissions_forecast=DFBackend(forecast_df.data['emissions'], forecast_df.data['timestamps']),\n",
    "    carbon_emissions_actual=DFBackend(actual_df.data['emissions'], actual_df.data['timestamps']),\n",
    "    carbon_prices_forecast=DFBackend(forecast_df.data['prices'], forecast_df.data['timestamps']),\n",
    "    carbon_prices_actual=DFBackend(actual_df.data['prices'], actual_df.data['timestamps']),\n",
    "    volume_forecast=DFBackend(None, None),\n",
    "    volume_actual=DFBackend(None, None),\n",
    ")\n",
    "\n",
    "#instantiate a test grid Object by feeding in the parsed test data as an argument to the Grid class\n",
    "grid = Grid(\n",
    "    timestep,\n",
    "    \"Grid\",\n",
    "    len(storage_capacity) + 2,\n",
    "    \"Simple Market as a Grid\",\n",
    "    \"*/5 * * * *\",\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    grid_data,\n",
    ")\n",
    "\n",
    "#modify the training data to test data and run the inference:\n",
    "env.decision_unit.markets[0] = grid\n",
    "env.decision_unit.generate_schedule(\n",
    "        current_reference_time=np.datetime64(pd.Timestamp(forecast_df.data['timestamps'][0]))\n",
    "    )\n",
    "env.start_time = forecast_df.data['timestamps'][0]\n",
    "env.decision_unit.storage_entities[0].current_soc = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test results for MILP\n",
    "# Code for MILP to generate results on the training dataset: \n",
    "if milp_flag:\n",
    "\n",
    "    # instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = MILPBattOpt(\n",
    "        env=env, objective=weight_price, solver=solver[0], seed=seed\n",
    "    )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    # provide the model to the environment so as to prepare the constraints and objectives\n",
    "    env.set_model(model)\n",
    "\n",
    "    # test the MILP model\n",
    "    print(\"-------Producing results on test set--------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    testdf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    testdf.to_csv(dir_path+\"testdf_MILP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results for RL test data\n",
    "if not milp_flag:  \n",
    "    \n",
    "    #load the model first, if trained from the python script training_RL.py \n",
    "    opt.load('model_checkpoints_UK/', 'Trained_RL_model')\n",
    "      \n",
    "    print(\"....... Starting Inference on the test dataset ........\")\n",
    "    #test the RL model on the test set\n",
    "    net_rewardt,rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(opt.model, env, opt, milp_flag)\n",
    "\n",
    "    # dump the test results into a csv file\n",
    "    testdf= create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    testdf.to_csv(dir_path+\"testdf_RL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization through Streamlit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed the following command to run streamlit in the background that would show the results\n",
    "#modify the arguments as and when required, feed the training csv file, test csv file and whether the files are solved using MILP/RL. \n",
    "import os\n",
    "pid=os.system(f\"python -m streamlit run streamlit_analyze.py --server.address 127.0.0.1 --server.port 8501 -- --weight_emission {weight_emission} --weight_price {weight_price} &\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='http://127.0.0.1:8501',  width=1000, height=1050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('encortex')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77375ee3feb625979ad554bacceec151ce55face344385276075fcd43c126865"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
