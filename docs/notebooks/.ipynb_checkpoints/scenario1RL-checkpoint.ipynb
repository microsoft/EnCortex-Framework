{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 1: Solving Battery Arbitrage Problem (Price and Carbon Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our partner, a global energy provider is deploying Li-ion batteries at their consumer premises to either maximize economic profit (increase cost savings by charging/discharging the battery at off-peak/peak price periods) or maximize environmental impact (reduce overall carbon footprint by charging/discharging the battery based on usage of renewable/non-renewable energy sources). \n",
    "\n",
    "Thus, it is important to optimize the management and scheduling of these batteries (when to charge or discharge) to maximize the objective.\n",
    "\n",
    "<img src='../_static/energy_arbitrage.png' height = 200 width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Import the libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, first we import all the general as well as encortex based abstractions necessary to solve the problem statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required general libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set() \n",
    "import typing as t \n",
    "import gym\n",
    "from gym import spaces\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import ipywidgets as widgets\n",
    "from itertools import repeat\n",
    "InteractiveShell.ast_node_interactivity=\"all\"\n",
    "\n",
    "#import the encortex library and all the required dependencies\n",
    "from encortex.backend import DFBackend\n",
    "from encortex.env import EnCortexEnv\n",
    "from encortex.logger import get_experiment_logger\n",
    "from encortex.utils.data_loaders import load_data\n",
    "from encortex.data import MarketData\n",
    "from encortex.contract import Contract\n",
    "from encortex.decision_unit import DecisionUnit\n",
    "from encortex.grid import Grid\n",
    "from encortex.sources import Battery, BatteryAction\n",
    "\n",
    "from dfdb import create_in_memory_db\n",
    "\n",
    "from encortex.environments import BatteryArbitrageScenarioEnv\n",
    "from encortex.optimizers import DRLBattOpt, MILPBattOpt, SimulatedAnnealingOpt\n",
    "from encortex.datasets.grid import CaliforniaPricesEmissionsData, UKPricesEmissionsData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Inputs from the User "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we present certain configurable parameters, that the user can tweak and experiment to improve the performance for the scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Optimization Algorithms :  We support multiple algorithms such as ,\n",
    "    - Mixed Integer Linear Programming (MILP)\n",
    "    - Simulated Annealing (SA)\n",
    "    - Deep Reinforcement Learning (DRL)\n",
    "\n",
    "    The user can use the following flags to specify the type of algorithm to be used and mention the solver name to run the optimization. \n",
    "    \n",
    "    The following cell shows how to run Reinforcement Learning. Deep Q-Networks (dqn) is used for the problem statement given here. We support multiple other reinforcement learning algorithms like :  Advantage Actor Critic (a2c), Proximal Policy Optimization (PPO) and so on. Therefore the respective solver names to be used are :  \"dqn\", \"a2c\", \"PPO\". Check for all the optimizers that can be used from [here](../encortex/encortex.optimizers.battery_arbitrage_optimizer.rst).\n",
    "\n",
    "    There are various solvers which can be used for MILP. We support : OR-Tools (\"ort\"), Gurobi (\"grb\"), Cplex (\"cpx\"), CyLP (\"clp\"), ECOS (\"eco\"), MOSEK (\"msk\") and so on. We recommend using OR-Tools as a free open source solver producing similar reproducible correct solution. Gurobi is the other recommended solver which although commercial, takes lesser solving time to produce similar result. \n",
    "\n",
    "    Simulated Annealing doesnot require any solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the type of optimization to be used:\n",
    "milp_flag = False \n",
    "simulated_annealing_flag = False\n",
    "solver = [\"dqn\"] #the algorithm to be used\n",
    "\n",
    "# # To use MILP:  \n",
    "# milp_flag = True \n",
    "# simulated_annealing_flag = False\n",
    "# solver = [\"grb\"] #the algorithm to be used\n",
    "\n",
    "# # To use SA:\n",
    "# milp_flag = False \n",
    "# simulated_annealing_flag = True\n",
    "# solver = [\"\"] #the algorithm to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Selection of Objectives: An user can choose to optimize for any combination of the following objectives by providing the relative importance weights as a float value:\n",
    "    - Carbon Optimization\n",
    "    - Price Optimization\n",
    "\n",
    "    For example, in the following cell, equal importance has been given to emission and price values, thus the algorithms will optimize for both the objectives leading to optimal schedules that the energy operator can take by which both increasing profits and reducing carbon footprints can be taken care of.\n",
    "\n",
    "    Because of the high variability in the data, it is not always intuitive to provide equal importance to both emissions and prices so as to lead to optimal savings. For this, we use Pareto Optimization curves to come to a single point of optimality, as discussed in our Paper.\n",
    "    \n",
    "    Since batteries perform a limited number of cycles during their lifetime, we consider an accurate battery degradation model to model the battery's lifetime. Hence, the Degradation importance weightage is also provided in addition to the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide optimization weights for the objectives\n",
    "weight_emission = 1.0\n",
    "weight_price = 1.0\n",
    "weight_degradation = 0.0\n",
    "\n",
    "# # Cost Optimization\n",
    "# weight_emission = 0.0\n",
    "# weight_price = 1.0\n",
    "# weight_degradation = 0.0\n",
    "\n",
    "# # Carbon Optimization\n",
    "# weight_emission = 1.0\n",
    "# weight_price = 0.0\n",
    "# weight_degradation = 0.0\n",
    "\n",
    "# # Using Degradation + joint optimization\n",
    "# weight_emission = 1.16\n",
    "# weight_price = 1.0\n",
    "# weight_degradation = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Battery Configurations: An user can run several experiments by tweaking the battery configurations. Following are the battery configurations which are left to user for configurable inputs:\n",
    "\n",
    "    - storage_capacity : the battery capacity (in kWh)\n",
    "    - efficiency : here, charging and discharging efficiency (in %) taken the same/ if different take it differently \n",
    "    - depth_of_discharge : the maximum discharge (in %) percentage that can happen at a time, here 90%\n",
    "    - soc_minimum : the minimum state of charge of the battery, below which the battery should not be explored\n",
    "    - timestep : battery decision time steps\n",
    "    - degradation_flag : whether to have degradation model in place or not for the batteries \n",
    "    - min_battery_capacity_factor : the battery capacity reduction percentage due to degradation, below which if capacity reduces due to overuse, battery doesnot stay at good optimal health\n",
    "    - battery_cost_per_kWh : the battery replacement cost (in $/kWh)\n",
    "    - reduction_coefficient : after every charge-discharge cycles over a certain period, the battery capacity reduced by the reduction coefficienct \n",
    "    - degradation_period_in_days : the period after which battery degrades\n",
    "    - action : battery actions\n",
    "    - soc_initial : initial state of charge of the battery to run the test experiments\n",
    "    - test_flag : the flag initiates random initial state of charge of the battery during training runs/experiments to avoid overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrun experiments for the following battery configurations\\nelements in the list denote different batteries/battery configurations to be used in the scenario together (here just 1 element indicating 1 battery being used)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "run experiments for the following battery configurations\n",
    "elements in the list denote different batteries/battery configurations to be used in the scenario together (here just 1 element indicating 1 battery being used)\n",
    "'''\n",
    "storage_capacity = [10.]\n",
    "efficiency=[1.]\n",
    "depth_of_discharge = [90.] \n",
    "soc_minimum = [0.1] \n",
    "timestep = [np.timedelta64(\"1\",\"h\")]\n",
    "degradation_flag = weight_degradation > 0 \n",
    "min_battery_capacity_factor = [0.8] \n",
    "battery_cost_per_kWh = [200.] \n",
    "reduction_coefficient = [0.99998] \n",
    "degradation_period_in_days = [7.] \n",
    "action = [BatteryAction(\"CHARGE_IDLE_DISCHARGE\",\"actions of the battery\",spaces.Discrete(3),True,)] \n",
    "\n",
    "soc_initial = [0.5] \n",
    "test_flag = [False] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Instantiating Objects of the required abstractions from the framework:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, the energy operator determines the entities involved in the scenario and uses the framework provided abstractions for the same. Following are the two entities used here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Battery Entity : \n",
    "We inherit the storage class to define a Li-ion battery entity. In this scenario, we define three battery actions: charge at max rate, discharge at max rate or stay idle. The energy operator populates the parameter values based on their battery configuration and instantiate an EnCortex-Battery object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninstantiate battery objects into a list based on the no. of batteries/elements provided in the list of configuration parameters \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10139/2074561833.py:21: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  degradation_period=(degradation_period_in_days[ele] * 24 * (np.timedelta64(60, 'm') / timestep[0])).astype(np.int),\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "instantiate battery objects into a list based on the no. of batteries/elements provided in the list of configuration parameters \n",
    "'''\n",
    "batteries = []\n",
    "for ele in range(len(storage_capacity)):\n",
    "    battery = Battery(\n",
    "        timestep=timestep,\n",
    "        name=\"Li-Ion Battery\",\n",
    "        id=ele,\n",
    "        description=\"Li-Ion Battery\",\n",
    "        storage_capacity=storage_capacity[ele],\n",
    "        charging_efficiency=efficiency[ele],\n",
    "        discharging_efficiency=efficiency[ele],\n",
    "        soc_initial=soc_initial[ele],\n",
    "        depth_of_discharge=depth_of_discharge[ele],\n",
    "        soc_minimum=soc_minimum[ele],\n",
    "        degradation_flag=degradation_flag,\n",
    "        min_battery_capacity_factor=min_battery_capacity_factor[ele],\n",
    "        battery_cost_per_kWh=battery_cost_per_kWh[ele],\n",
    "        reduction_coefficient=reduction_coefficient[ele],\n",
    "        degradation_period=(degradation_period_in_days[ele] * 24 * (np.timedelta64(60, 'm') / timestep[0])).astype(np.int),\n",
    "        test_flag=test_flag[ele],\n",
    "        action=action[ele],\n",
    "    )\n",
    "    batteries.append(battery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Simplified real-time market entity as Grid: Since Energy Arbitrage does not require any bidding decisions in the market, we modify the real-time market entity to a simplified real-time market entity that captures just the real-time market prices along with the carbon footprint information. This shows the utility of our abstractions, which allow seamless modification/extension of the definitions based on the scenario. Check into the encortex references to know more about the argument details.\n",
    "\n",
    "\n",
    "Now, this entity requires loading data. There are two ways of using the data:\n",
    "\n",
    "- __Download data__ from any public source (here, we share an onedrive [link](https://microsoftapc-my.sharepoint.com/:f:/g/personal/t-vballoli_microsoft_com/Evd4JIo7F4hFjI9Y_MJPVEYBYc4iP2i-OND1gfoCx3xiIQ?e=hhzg4M) to show the functionality of the same), create a folder named data and add your train.csv and test.csv files for both forecast and actual data.   \n",
    "\n",
    "- Using __Data Loaders__ of Encortex: We have some publicly available data support in the framework.(The commented section shows the use of Data Loaders here). Also, one needs to replace all the forecast_df and actual_df with forecast_df.data and actual_df.data here. The existing data loaders takes in 3 user-specific arguments:\n",
    "    - train: A flag saying whether training/test data to load\n",
    "    - forecasts: A flag saying whether experiments are to be run on forecasts/actuals\n",
    "    - forecast_type: A string specifyng the type of forecast:\n",
    "        - noise : Adding noise to the actual values and treating that as forecasts\n",
    "        - smoothing : Smoothing the actual values and using that as forecasts\n",
    "        - yesterdays : assuming yesterday's actual data as forecasts for today's data\n",
    "        - meanprev : assuming mean of previous n days as forecasts for today's data (default)\n",
    "        - lgbm : using light gradient boosting machine to produce forecasts\n",
    "        - nbeats: using nbeats model to produce forecasts\n",
    "        - auto : if forecasts already available load that instead\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntraining data is not required for MILP and Simulated Annealing, if working on RL training data is required, read the data from the dataloader, \\nparse it to the backend of MArketData and feed it to the Grid class to instantiate a Grid object for training\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "training data is not required for MILP and Simulated Annealing, if working on RL training data is required, read the data from the dataloader, \n",
    "parse it to the backend of MArketData and feed it to the Grid class to instantiate a Grid object for training\n",
    "'''\n",
    "\n",
    "forecast_df = pd.read_csv(\"data/UK_data_2018_2019_meanprev.csv\")\n",
    "actual_df = pd.read_csv(\"data/UK_data_2018_2019_actuals.csv\")\n",
    "\n",
    "# forecast_df = UKPricesEmissionsData(train=True, forecasts=True, forecast_type=\"meanprev\")\n",
    "# actual_df = UKPricesEmissionsData(train=True, forecasts=False)\n",
    "\n",
    "forecast_df[['emissions', 'prices']]  = forecast_df[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "actual_df[['emissions', 'prices']]  = actual_df[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "\n",
    "#parse the training data to the MarketData backend\n",
    "grid_data = MarketData.parse_backend(\n",
    "    entity_id = len(storage_capacity) + 2,\n",
    "    in_memory = True,\n",
    "    market_id = len(storage_capacity) + 2,\n",
    "    entity_forecasting_id = len(storage_capacity) + 2,\n",
    "    timestep = np.timedelta64(\"5\", \"m\"),\n",
    "    price_forecast=DFBackend(forecast_df['prices'], forecast_df['timestamps']),\n",
    "    \n",
    "    price_actual=DFBackend(actual_df['prices'], actual_df['timestamps']),\n",
    "    carbon_emissions_forecast=DFBackend(forecast_df['emissions'], forecast_df['timestamps']),\n",
    "    carbon_emissions_actual=DFBackend(actual_df['emissions'], actual_df['timestamps']),\n",
    "    carbon_prices_forecast=DFBackend(forecast_df['prices'], forecast_df['timestamps']),\n",
    "    carbon_prices_actual=DFBackend(actual_df['prices'], actual_df['timestamps']),\n",
    "    volume_forecast=DFBackend(None, None),\n",
    "    volume_actual=DFBackend(None, None),\n",
    ")\n",
    "\n",
    "#instantiate a training grid Object by feeding in the parse training data as an argument to the Grid class\n",
    "grid = Grid(\n",
    "    timestep=timestep,\n",
    "    name = \"Grid\",\n",
    "    id = len(storage_capacity) + 2,\n",
    "    description = \"Simple Market as a Grid\",\n",
    "    bid_start_time_schedule = \"*/5 * * * *\",\n",
    "    bid_window = np.timedelta64(5, \"m\"),\n",
    "    commit_start_schedule = np.timedelta64(5, \"m\"),\n",
    "    commit_end_schedule = np.timedelta64(5, \"m\"),\n",
    "    data = grid_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Creating Decision units for the problem statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision units are built based on the entities and contracts associated with a particular producer. Contracts define the flow of energy between 2 entities in the framework. We use a graph representation of entities as nodes and contracts as edges to identify decision units. A decision unit generates critical information on the schedule and the associated actions based on the included contracts/entities.\n",
    "\n",
    "Here, for this scenario, contracts are between the grid and the batteries installed near to the consumer, and the decision unit is built on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate the decision unit (both for training and testing) by creating contracts between the grid and the battery\n",
    "def creating_decision_units(batteries, grid, forecast_df):\n",
    "    contracts = []\n",
    "    for battery in batteries:\n",
    "        contracts.append(Contract(grid,battery))\n",
    "    decision_unit = DecisionUnit(contracts)\n",
    "    decision_unit.generate_schedule(\n",
    "        current_reference_time=np.datetime64(pd.Timestamp(forecast_df['timestamps'][0]))\n",
    "    )\n",
    "    return decision_unit\n",
    "\n",
    "decision_unit = creating_decision_units(batteries, grid, forecast_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Function to Store Results in a dataframe and then later to csv format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump results into dataframes for later visualizations - \n",
    "- battery_soc_list: stores the current list of state of charge values for MILP\n",
    "- action_list : list of actions: charging/discharging/idle taken by the optimizer for a step\n",
    "- power_list: power associated with the action taken by the optimization algorithm\n",
    "- carbon_intensity_list: Actual carbon emission intensity values in gCO2eq/kWh\n",
    "- price_intensity_list: Actual price values in $\n",
    "- reward_list: List of rewards received for taking actions in particular states\n",
    "- carbon_savings_forecast_list : Carbon savings done due to the action taken for a particular state at a certain timestamp\n",
    "- price_savings_forecast_list : Price savings due to the action taken for a particular state at a certain timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(\n",
    "    battery_soc_list, \n",
    "    action_list, \n",
    "    power_list, \n",
    "    carbon_intensity_list,\n",
    "    price_intensity_list,\n",
    "    carbon_savings_list,\n",
    "    price_savings_list,\n",
    "    reward_list, \n",
    "    carbon_savings_forecast_list, \n",
    "    price_savings_forecast_list\n",
    "):\n",
    "    df=pd.DataFrame()\n",
    "    df.insert(loc=0, column='Current_SOC', value=battery_soc_list)\n",
    "    df.insert(loc=1, column='Predicted_Action', value=action_list)\n",
    "    df.insert(loc=2, column='Predicted_Power_Action', value=power_list)\n",
    "    df.insert(loc=3, column='Carbon_emissions', value=carbon_intensity_list)\n",
    "    df.insert(loc=4, column='Carbon_savings', value=carbon_savings_list)\n",
    "    df.insert(loc=5, column='Forecast_Carbon_savings', value=carbon_savings_forecast_list)\n",
    "    df.insert(loc=6, column='Price_emissions', value=price_intensity_list)\n",
    "    df.insert(loc=7, column='Price_savings', value=price_savings_list)\n",
    "    df.insert(loc=8, column='Forecast_Price_savings', value=price_savings_forecast_list)\n",
    "    df.insert(loc=9, column='Reward', value=reward_list)  \n",
    "    return df\n",
    "\n",
    "#store the results into results folder\n",
    "country = \"UK\" # change it to the respective country name based on the grid's price/emissions data\n",
    "dir = os.getcwd()\n",
    "dir_path = os.path.join(dir, f'results_{country}/')\n",
    "\n",
    "if not os.path.isdir(dir_path):\n",
    "    os.mkdir(dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Instantiate the environment object from the scenario specific enironment class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment forms a key leyer in the EnCortex architecture to provide data and state information (state space) from entities that are needed to make a decision (action space) and a central point to orchestrate all the required decisions based on the schedule. \n",
    "\n",
    "EnCortex supports some of the common scenario based environments which can be easily extended to other similar custom scenarios by the energy operators. BatteryArbitrageScenarioEnv is one of the supported environments by EnCortex. Check here to know more details. The step_time_difference is another user-configurable parameter which says about the optimization step to be taken. For MILP, the optimum result comes when the step time difference is set similar to the timestep parameter. For Reinforcement Learning, it is a mandate to set it equal to the timestep else the action size increases which leads to errorneous learning by the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instantiate an environment oject from the scenario specific environment class \n",
    "'''\n",
    "if milp_flag or simulated_annealing_flag:\n",
    "    step_time_diff = np.timedelta64(\"1\", \"D\")\n",
    "else:\n",
    "    step_time_diff = np.timedelta64(\"1\", \"h\")\n",
    "\n",
    "if simulated_annealing_flag:\n",
    "    continuous = True\n",
    "else:\n",
    "    continuous = False\n",
    "\n",
    "env = BatteryArbitrageScenarioEnv(\n",
    "    decision_unit,\n",
    "    start_time=forecast_df['timestamps'][0],\n",
    "    timestep=np.timedelta64(\"1\", \"h\"),\n",
    "    step_time_difference=step_time_diff,\n",
    "    horizon=np.timedelta64(\"1\", \"D\"),\n",
    "    seed=40,\n",
    "    weight_emission=weight_emission,\n",
    "    weight_degradation=weight_degradation,\n",
    "    weight_price=weight_price,\n",
    "    logging_interval = 1,\n",
    "    continuous=continuous\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Training Pipeline for the algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is only for RL and testing code can be split into 3 sections of RL, MILP, SA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, set the seed. This helps reproucing the results in the same machine, but still across different machine it doesnot guarantee to produce same result. The extreme noisy learning pattern, large amount of hyperparameter tuning, unpredictability and unexplainability of the RL agents add to the demerits of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting a seed for reproducibility of experiment results:\n",
    "pl.seed_everything(40)\n",
    "seed = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training in RL begins here, where we instantiate the DRLBattOpt based optimizer object and then save the best trained model to automatically created model_checkpoints folder for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTraining Pipeline for RL:\\nThe code in this cell helps to train a RL model, but there could be issues in trying it out in the jupyter cell. \\nHence we also provide a separate training script namely training_RL.py, try that out if the jupyter cell doesnot work.\\n\\nDuring testing just the load the model saved from the training script\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "...... Starting Training .......\n",
      "------Training Completed--------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training Pipeline for RL:\n",
    "The code in this cell helps to train a RL model, but there could be issues in trying it out in the jupyter cell. \n",
    "Hence we also provide a separate training script namely training_RL.py, try that out if the jupyter cell doesnot work.\n",
    "\n",
    "During testing just the load the model saved from the training script\n",
    "'''\n",
    "if not (milp_flag or simulated_annealing_flag):\n",
    "\n",
    "    #instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = DRLBattOpt(\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    print(\"...... Starting Training .......\")\n",
    "    if not os.path.exists(f\"model_checkpoints/best_model.zip\"):\n",
    "        model = opt(\n",
    "            env,\n",
    "            train_flag=True,\n",
    "            path = 'model_checkpoints'\n",
    "\n",
    "        )\n",
    "    print(\"------Training Completed--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed integer linear programming (MILP) and Simulated Annealing doesnot require training, hence the code for the MILP and SA section is shown directly while testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Testing Pipeline for the algorithms :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is a sample general testing function which is robust to all the optimizers supported by the framework which returns the rewards accumulated and other savings specific variables to help in visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The test pipeline:\n",
    "def testing(model, env: BatteryArbitrageScenarioEnv, opt, milp_flag: bool, simulated_annealing:bool):\n",
    "    \n",
    "    #for logging results into csv files\n",
    "    carbon_intensity_list=[]\n",
    "    price_intensity_list=[]\n",
    "    battery_soc_list=[]\n",
    "    action_list=[]\n",
    "    power_list=[]\n",
    "    reward_list=[]\n",
    "    carbon_savings_list=[]\n",
    "    price_savings_list=[]\n",
    "    carbon_savings_forecast_list=[]\n",
    "    price_savings_forecast_list=[]\n",
    "    \n",
    "    #For testing initiate the battery with 50% charge always (initial state of charge of the battery during test experiments : 0.5)\n",
    "    for batt in env.decision_unit.storage_entities:\n",
    "        batt: Battery\n",
    "        batt.current_soc = 0.5\n",
    "        batt.test = True\n",
    "        \n",
    "    #Reset the environment in the beginning and get the state values\n",
    "    state = env.reset()\n",
    "    done = env.is_done\n",
    "    net_reward = 0\n",
    "    steps=0\n",
    "    \n",
    "    #run the episode unless done\n",
    "    while not done:\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"State: \", len(state))\n",
    "        print(\"Step no : \", steps)\n",
    "        print(\"Battery SOC : \", env.decision_unit.storage_entities[0].current_soc)\n",
    "        #storing the actual values of emissions & prices in the list for logging purpose\n",
    "        for grid in env.decision_unit.markets:\n",
    "            grid:Grid\n",
    "            carbon_intensity_list+=list(grid.data.carbon_emissions_actual[env.time, env.time+env.step_time_difference].reshape(-1))\n",
    "            price_intensity_list+=list(grid.data.carbon_prices_actual[env.time, env.time+env.step_time_difference].reshape(-1))\n",
    "         \n",
    "        #storing the state of charge of the batteries in a list for logging purpose:\n",
    "        for batt in env.decision_unit.storage_entities:\n",
    "            batt: Battery\n",
    "            battery_soc_list+=[batt.current_soc]\n",
    "             \n",
    "        if milp_flag or simulated_annealing:\n",
    "            #In MILP first the values are passed as a decision variable/ Affine Expression - the train flag signifies that\n",
    "            env.train_flag = True\n",
    "            \n",
    "            #model called to solve the objective defined in the environment based on the constraints from the framework abstractions\n",
    "            model = opt(train_flag=True)\n",
    "            battery_actions = opt.predict(env)\n",
    "            \n",
    "            #get the numeric action values as the predicted action results and hence switch off the train flag\n",
    "            env.train_flag = False\n",
    "            \n",
    "            for batt in env.decision_unit.storage_entities:\n",
    "                batt: Battery\n",
    "                if not milp_flag:\n",
    "                    action_dict = {}\n",
    "                    action = np.round(battery_actions*3 - 0.5)\n",
    "                    action_dict[batt.id] = {\"time\": env.time, \"action\": action}\n",
    "                    battery_actions = env.transform(action_dict)\n",
    "\n",
    "                action_list+=list(battery_actions[batt.id]['Dt']-battery_actions[batt.id]['Ct']+1)\n",
    "                power_list+=list((battery_actions[batt.id]['Dt']-battery_actions[batt.id]['Ct'])*batt.max_discharging_power)\n",
    "        else:\n",
    "            #In RL, since training is already done, just load the model to predict the actions based on the current state\n",
    "            battery_actions = model.predict(state)[0]\n",
    "\n",
    "            #transform the actions similar to the environment-specific action transformation for uniformity\n",
    "            for batt in env.decision_unit.storage_entities:\n",
    "                #storing the power values into a list for logging purpose\n",
    "                power_list.append((battery_actions -1)*batt.max_discharging_power)\n",
    "\n",
    "            # storing actions taken into a list for logging purpose:\n",
    "            action_list+=[battery_actions]\n",
    "        \n",
    "        #Step to the next_state, based on the actions predicted by the optimizers, getting a reward and indicating whether the episode completed or not\n",
    "        # print(\"Battery Actions:\", battery_actions)\n",
    "        next_state, reward, done, info = env.step(battery_actions)\n",
    "        \n",
    "        #storing the rest of the state of charges in milp of the batteries in a list for logging purpose:\n",
    "        if milp_flag or simulated_annealing:\n",
    "            if env.step_time_difference == env.horizon:\n",
    "                for batt in env.decision_unit.storage_entities:\n",
    "                    batt: Battery\n",
    "                    battery_soc_list+=info[batt.id]['soc_list'][:-1]\n",
    "        \n",
    "            #storing reward values in a list for logging purpose:\n",
    "            reward_list+=[0]*(int(env.step_time_difference / env.timestep)-1)\n",
    "        reward_list+=[reward]\n",
    "        \n",
    "        net_reward += reward\n",
    "        state = next_state\n",
    "        print(\"Total reward\", net_reward)\n",
    "        \n",
    "        #storing the savings values in the lists for logging purposes\n",
    "        carbon_savings_list.append(env.carbon_savings_list)\n",
    "        price_savings_list.append(env.price_savings_list)\n",
    "        carbon_savings_forecast_list.append(env.carbon_savings_forecast_list)\n",
    "        price_savings_forecast_list.append(env.price_savings_forecast_list)\n",
    "        \n",
    "        steps+=1\n",
    "    return net_reward, reward_list, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list[0], price_savings_list[0], carbon_savings_forecast_list[0], price_savings_forecast_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The following code snippet generates results on the training dataset using MILP optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for MILP to generate results on the training dataset: \n",
    "if milp_flag:\n",
    "\n",
    "    # instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = MILPBattOpt(\n",
    "        env=env, objective=weight_price, solver=solver[0], seed=seed\n",
    "    )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    # provide the model to the environment so as to prepare the constraints and objectives\n",
    "    env.set_model(model)\n",
    "\n",
    "    # test the MILP model\n",
    "    print(\"-------Producing results on training set--------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    traindf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    traindf.to_csv(dir_path+\"traindf_MILP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The following code snippet generates results on the training dataset using Simulated Annealing Optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Simulated Annealing to generate results on the training set:\n",
    "if simulated_annealing_flag:\n",
    "\n",
    "    #instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = SimulatedAnnealingOpt(\n",
    "        env = env, objective= weight_price, seed = seed\n",
    "    )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    #test the Simulated Annealing model\n",
    "    print(\"-----------Producing results on training set------------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    traindf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    traindf.to_csv(dir_path+\"traindf_SA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, we load the saved trained DRL model to generate inference results on the same training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Trained RL model on the training data set first:\n",
    "if not (milp_flag or simulated_annealing_flag): \n",
    "\n",
    "    opt = DRLBattOpt(\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    #load the model first, if trained from the python script training_RL.py \n",
    "    opt.load('model_checkpoints/', 'best_model')\n",
    "    \n",
    "    \n",
    "    print(\"-------Producing results on training set--------\")\n",
    "    #test the RL model on the training set\n",
    "    net_rewardt,rewardlist,  battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(opt.model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the training results into csv file\n",
    "    traindf= create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list,rewardlist,  carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    traindf.to_csv(dir_path+\"traindf_DRL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Test Data and Reinitialization: \n",
    "Now, we need to reinitiailize the grid object with the test dataset and change the respective decision unit contracts from the environment, so as to make the environment test ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read test data having emissions & prices values of dataset\n",
    "#load forecast and actual data for testing/inference \n",
    "forecast_df = pd.read_csv(\"data/UK_data_2020_meanprev.csv\")\n",
    "actual_df = pd.read_csv(\"data/UK_data_2020_actuals.csv\")\n",
    "\n",
    "# forecast_df = UKPricesEmissionsData(train=False, forecasts=True) \n",
    "# actual_df = UKPricesEmissionsData(train=False, forecasts=False)\n",
    "\n",
    "forecast_df[['emissions', 'prices']]  = forecast_df[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "actual_df[['emissions', 'prices']]  = actual_df[['emissions', 'prices']].apply(lambda x: np.float32(x))\n",
    "\n",
    "#parse the test data to the MarketData backend\n",
    "grid_data = MarketData.parse_backend(\n",
    "    len(storage_capacity) + 2,\n",
    "    True,\n",
    "    len(storage_capacity) + 2,\n",
    "    len(storage_capacity) + 2,\n",
    "    np.timedelta64(\"5\", \"m\"),\n",
    "    price_forecast=DFBackend(forecast_df['prices'], forecast_df['timestamps']),\n",
    "    price_actual=DFBackend(actual_df['prices'], actual_df['timestamps']),\n",
    "    carbon_emissions_forecast=DFBackend(forecast_df['emissions'], forecast_df['timestamps']),\n",
    "    carbon_emissions_actual=DFBackend(actual_df['emissions'], actual_df['timestamps']),\n",
    "    carbon_prices_forecast=DFBackend(forecast_df['prices'], forecast_df['timestamps']),\n",
    "    carbon_prices_actual=DFBackend(actual_df['prices'], actual_df['timestamps']),\n",
    "    volume_forecast=DFBackend(None, None),\n",
    "    volume_actual=DFBackend(None, None),\n",
    ")\n",
    "\n",
    "#instantiate a test grid Object by feeding in the parsed test data as an argument to the Grid class\n",
    "grid = Grid(\n",
    "    timestep,\n",
    "    \"Grid\",\n",
    "    len(storage_capacity) + 2,\n",
    "    \"Simple Market as a Grid\",\n",
    "    \"*/5 * * * *\",\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    np.timedelta64(5, \"m\"),\n",
    "    grid_data,\n",
    ")\n",
    "\n",
    "#modify the training data to test data and run the inference:\n",
    "env.decision_unit.markets[0] = grid\n",
    "env.decision_unit.generate_schedule(\n",
    "        current_reference_time=np.datetime64(pd.Timestamp(forecast_df['timestamps'][0]))\n",
    "    )\n",
    "env.start_time = (pd.Timestamp(forecast_df['timestamps'][0]))\n",
    "env.decision_unit.storage_entities[0].current_soc = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Similar to the training inference pipeline, we use the same code snippet to generate results on the test dataset using MILP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate test results for MILP\n",
    "# Code for MILP to generate results on the training dataset: \n",
    "if milp_flag:\n",
    "\n",
    "    # instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = MILPBattOpt(\n",
    "        env=env, objective=weight_price, solver=solver[0], seed=seed\n",
    "    )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    # provide the model to the environment so as to prepare the constraints and objectives\n",
    "    env.set_model(model)\n",
    "\n",
    "    # test the MILP model\n",
    "    print(\"-------Producing results on test set--------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    testdf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    testdf.to_csv(dir_path+\"testdf_MILP.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Similar to the training inference pipeline, we use the same code snippet to generate results on the test dataset using SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test results for Simulated Annealing\n",
    "# Code for SA to generate results on the testing dataset:\n",
    "\n",
    "if simulated_annealing_flag:\n",
    "    \n",
    "    #instantiate an optimizer object based on the optimizer chosen, and create the model\n",
    "    opt = SimulatedAnnealingOpt(\n",
    "            env=env, objective=weight_price, seed=seed\n",
    "        )\n",
    "    model = opt(train_flag=True)\n",
    "\n",
    "    #test the Simulated Annealing model\n",
    "    print(\"-------Producing results on testing set--------\")\n",
    "    net_rewardt, rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(\n",
    "        model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the results into csv file\n",
    "    testdf = create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list,\n",
    "                          carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    testdf.to_csv(dir_path+\"testdf_SA.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Similar to the training inference pipeline, we use the same code snippet to generate results on the test dataset using DRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate results for RL test data\n",
    "if not (milp_flag or simulated_annealing_flag):  \n",
    "    \n",
    "    #load the model first, if trained from the python script training_RL.py \n",
    "    opt.load('model_checkpoints/', 'best_model')\n",
    "      \n",
    "    print(\"....... Starting Inference on the test dataset ........\")\n",
    "    #test the RL model on the test set\n",
    "    net_rewardt,rewardlist, battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, carbon_savings_forecast_list, price_savings_forecast_list = testing(opt.model, env, opt, milp_flag, simulated_annealing_flag)\n",
    "\n",
    "    # dump the test results into a csv file\n",
    "    testdf= create_dataframe(battery_soc_list, action_list, power_list, carbon_intensity_list, price_intensity_list, carbon_savings_list, price_savings_list, rewardlist, carbon_savings_forecast_list, price_savings_forecast_list)\n",
    "    testdf.to_csv(dir_path+\"testdf_DRL.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Result Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the visualization object from the environment by passing 2 arguments:\n",
    "- results_folder : The local folder name, where all the final results are stored\n",
    "- optimizers : A list of optimizers for which results are present in the results_folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = env.visualize(results_folder = f\"results_{country}\",optimizers= [\"MILP\", \"DRL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after running the following cell, provide the following as input to visualize the plots:\n",
    "- A multiselect option to choose between optimizers, so as to compare the final savings between two or more of them. To multiselect pressShift+ leftClick.\n",
    "- Choose between training/test file options from the radio buttons provided to visualize the schedules generated for each of the optimizers running on the user-input option of train/test file.\n",
    "- From the slider, select a day for which the battery schedules are to be shown\n",
    "- Click on the Plot button to plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Savings over the whole dataset\n",
       "- No. of days in the train dataset : 728\n",
       "- No. of days in the test dataset : 364\n",
       "\n",
       " \n",
       "\n",
       " \n",
       "Choose an Optimizer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b1cc3de94e40dcb4204cd490da9b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Optimizer:', index=(0,), options=('MILP', 'DRL', 'SA'), value=('MILP',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " \n",
       "Choose the file to view results:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2953cabed45145f0a011ffc557c0a6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='File:', index=1, layout=Layout(width='max-content'), options=('Training File', 'Test…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       " \n",
       "Choose a day for checking schedules:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e9378e174d445cae661b4834be23ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Day:', max=728)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bdf76b63d54a81abf3d32c35558010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Plot', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebff0fc0071445abfcc54051d3addd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "menu = widgets.SelectMultiple(\n",
    "       options=['MILP', 'DRL', 'SA'],\n",
    "       value=['MILP'],\n",
    "       description='Optimizer:',\n",
    "       disabled = False)\n",
    "rdbutton = widgets.RadioButtons(\n",
    "            options=['Training File', 'Test File'],\n",
    "            value='Test File', \n",
    "            layout={'width': 'max-content'},\n",
    "            description='File:',\n",
    "            disabled=False\n",
    "        )\n",
    "\n",
    "slider = widgets.IntSlider(\n",
    "                value=0,\n",
    "                min=0,\n",
    "                max=int(vi.tr_files[list(menu.value)[0]].shape[0]/(env.horizon/env.timestep)),\n",
    "                step=1,\n",
    "                description = \"Day:\")\n",
    "\n",
    "button = widgets.Button(description='Plot')\n",
    "out = widgets.Output()\n",
    "def on_button_clicked(b):\n",
    "    with out:\n",
    "        clear_output()    \n",
    "        vi.initial_plots(menu.value)\n",
    "\n",
    "        if rdbutton.value ==\"Test File\":\n",
    "            if slider.value > int(vi.te_files[list(menu.value)[0]].shape[0]/(env.horizon/env.timestep)) :\n",
    "                print(\"Please choose a day lesser than Day 365, since its end of the test dataset\")\n",
    "                return\n",
    "\n",
    "        train_data = list(vi.tr_files.values()) \n",
    "        test_data = list(vi.te_files.values())\n",
    "        approach = list(menu.value)\n",
    "        if len(approach) == 1:\n",
    "            if approach[0] ==\"MILP\":\n",
    "                train_data=train_data[::2]\n",
    "                test_data=test_data[::2]\n",
    "            elif approach[0] == \"DRL\":\n",
    "                train_data=train_data[1::2]\n",
    "                test_data=test_data[1::2]\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if rdbutton.value == 'Training File':             \n",
    "            title = f'Results for the Day {slider.value} of UK Data'\n",
    "            vi.plot_results(train_data, slider.value, int(env.horizon/env.timestep), title, approach)\n",
    "        else:\n",
    "            title = f'Results for the Day {slider.value} of UK Data'\n",
    "            vi.plot_results(test_data, slider.value, int(env.horizon/env.timestep), title, approach)\n",
    "        \n",
    "button.on_click(on_button_clicked)\n",
    "info = display(Markdown(\"\"\"# Savings over the whole dataset\n",
    "- No. of days in the train dataset : {}\n",
    "- No. of days in the test dataset : {}\n",
    "\\n \n",
    "\\n \n",
    "Choose an Optimizer:\"\"\".format(int(vi.tr_files[list(menu.value)[0]].shape[0]/(env.horizon/env.timestep)),int(vi.te_files[list(menu.value)[0]].shape[0]/(env.horizon/env.timestep)))))\n",
    "display(menu)\n",
    "display(Markdown('''\\n \\nChoose the file to view results:'''))\n",
    "display(rdbutton)\n",
    "display(Markdown('''\\n \\nChoose a day for checking schedules:'''))\n",
    "display(slider, button, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial bar charts provide a \"Total Savings\" comparison between train and test files along with multiple optimizers if selected. The left hand side bar plots denote the overall cost savings, whereas the right side plots signify the carbon savings.\n",
    "\n",
    "The plot below gives a clear indication of how the state of charge of the battery (green coloured line charts for multiple optimizers if selected) varies with the repective variations in Price (yellow plot) and carbon emissions (red plot). Based on the objective selected by the user, the price and carbon variations play a key role in deciding the charging and discharging schedules. For example, a common inference drawn from the schedules plot is when the prices are high, the battery discharges, whereas when the prices are low, the battery tends to charge from the utility grid, thus maximizing the profit for the consumer. Similar conclusion can be drawn for carbon arbitrage as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77375ee3feb625979ad554bacceec151ce55face344385276075fcd43c126865"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04b1cc3de94e40dcb4204cd490da9b29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SelectMultipleModel",
      "state": {
       "_options_labels": [
        "MILP",
        "DRL",
        "SA"
       ],
       "description": "Optimizer:",
       "index": [
        0
       ],
       "layout": "IPY_MODEL_6204e856ac6a49b3afb2689c6e4f3240",
       "rows": 5,
       "style": "IPY_MODEL_970759b2973c4fac9784eb336f12a476"
      }
     },
     "275da07838c9425c889f94e8b974b684": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2953cabed45145f0a011ffc557c0a6b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "RadioButtonsModel",
      "state": {
       "_options_labels": [
        "Training File",
        "Test File"
       ],
       "description": "File:",
       "index": 1,
       "layout": "IPY_MODEL_5db6efd48cd84acea070db32144d166a",
       "style": "IPY_MODEL_5c654e4e0fd64797abded51a39646e5b"
      }
     },
     "2ebff0fc0071445abfcc54051d3addd5": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_a1e43c27d2b94ffcaa9b8456b1a12d6c"
      }
     },
     "38e9378e174d445cae661b4834be23ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntSliderModel",
      "state": {
       "description": "Day:",
       "layout": "IPY_MODEL_e7fa4255abaf4e7eb20a6cd135bd8e23",
       "max": 728,
       "style": "IPY_MODEL_600e8b8c57e74a98b3aaa08c3b7d95ca"
      }
     },
     "4f5fd4a9c8e7419489fb1998b802f650": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "LabelModel",
      "state": {
       "layout": "IPY_MODEL_f401c4b059974bd8818497dda27d66fe",
       "style": "IPY_MODEL_d1273c2fc51e4617bcada13009545d60"
      }
     },
     "5c654e4e0fd64797abded51a39646e5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5db6efd48cd84acea070db32144d166a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "max-content"
      }
     },
     "600e8b8c57e74a98b3aaa08c3b7d95ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6204e856ac6a49b3afb2689c6e4f3240": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7e52695116be499aa99af9e324c1f1de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_9322cc8944bc4d82af6d540500f6222d",
       "max": 1,
       "style": "IPY_MODEL_275da07838c9425c889f94e8b974b684"
      }
     },
     "7feeb515167b4f658a69eac59233ebda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4f5fd4a9c8e7419489fb1998b802f650",
        "IPY_MODEL_7e52695116be499aa99af9e324c1f1de"
       ],
       "layout": "IPY_MODEL_f6ca293b1bea4b6185c00ffe726683db"
      }
     },
     "80bdf76b63d54a81abf3d32c35558010": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Plot",
       "layout": "IPY_MODEL_97f85af090df4bee9f17c2e1ae9c6ed5",
       "style": "IPY_MODEL_ea8c342fb2ff4324b8f489d072f3ac22"
      }
     },
     "9322cc8944bc4d82af6d540500f6222d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "970759b2973c4fac9784eb336f12a476": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "97f85af090df4bee9f17c2e1ae9c6ed5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a1e43c27d2b94ffcaa9b8456b1a12d6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d1273c2fc51e4617bcada13009545d60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e7fa4255abaf4e7eb20a6cd135bd8e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ea8c342fb2ff4324b8f489d072f3ac22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "f401c4b059974bd8818497dda27d66fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f6ca293b1bea4b6185c00ffe726683db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
